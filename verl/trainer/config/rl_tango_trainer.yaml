defaults:
  - ppo_trainer
  - _self_

data:
  apply_chat_template: False

verifier:
  enable: true
  trainable: true
  hybrid_engine: true
  
  model:
    path: ~/models/deepseek-llm-7b-chat   # Match generator by default
    external_lib: null
    override_config: { }
    enable_gradient_checkpointing: true
    use_remove_padding: false
  
  actor:
    strategy: fsdp  # This is for backward-compatibility
    ppo_mini_batch_size: 256
    ppo_micro_batch_size: null # will be deprecated, use ppo_micro_batch_size_per_gpu
    ppo_micro_batch_size_per_gpu: null
    use_dynamic_bsz: false
    ppo_max_token_len_per_gpu: 16384
    grad_clip: 1.0
    clip_ratio: 0.2
    entropy_coeff: 0.001
    use_kl_loss: false
    use_torch_compile: true
    kl_loss_coef: 0.001
    kl_loss_type: low_var_kl
    ppo_epochs: 1
    shuffle: false
    ulysses_sequence_parallel_size: 1
    checkpoint:
      contents: ['model', 'hf_model', 'optimizer', 'extra']
    optim:
      lr: 1e-6
      lr_warmup_steps: -1
      lr_warmup_steps_ratio: 0.
      min_lr_ratio: null
      warmup_style: constant
      total_training_steps: -1
    fsdp_config:
      wrap_policy:
        min_num_params: 0
      param_offload: false
      optimizer_offload: false
      fsdp_size: -1
  
  ref:
    fsdp_config:
      param_offload: false
      wrap_policy:
        min_num_params: 0
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: null
    log_prob_use_dynamic_bsz: ${verifier.actor.use_dynamic_bsz}
    log_prob_max_token_len_per_gpu: ${verifier.actor.ppo_max_token_len_per_gpu}
    ulysses_sequence_parallel_size: ${verifier.actor.ulysses_sequence_parallel_size}
  
  rollout:
    name: vllm
    temperature: 1.0
    top_k: -1
    top_p: 1.0
    use_fire_sampling: false
    prompt_length: ${data.max_prompt_length}
    response_length: ${data.max_response_length}
    dtype: bfloat16
    gpu_memory_utilization: 0.5
    ignore_eos: false
    enforce_eager: true
    free_cache_engine: true
    load_format: dummy_dtensor
    tensor_model_parallel_size: 1
    max_num_batched_tokens: 8192
    max_model_len: null
    max_num_seqs: 1024
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: null
    log_prob_use_dynamic_bsz: ${verifier.actor.use_dynamic_bsz}
    log_prob_max_token_len_per_gpu: ${verifier.actor.ppo_max_token_len_per_gpu}
    disable_log_stats: true
    enable_chunked_prefill: true
    do_sample: true
    n: 1
    n_verifier_inference_rollouts: 1
    max_new_tokens: 2048
    val_kwargs:
      top_k: -1
      top_p: 1.0
      temperature: 0
      n: 1
      do_sample: false

verifier_critic:
  strategy: fsdp
  optim:
    lr: 1e-5
    lr_warmup_steps_ratio: 0.
    min_lr_ratio: null
    warmup_style: constant
    total_training_steps: -1
  model:
    path: ${verifier.model.path}
    tokenizer_path: ${verifier.model.path}
    override_config: { }
    external_lib: ${verifier.model.external_lib}
    enable_gradient_checkpointing: true
    use_remove_padding: false
    fsdp_config:
      param_offload: false
      optimizer_offload: false
      wrap_policy:
        min_num_params: 0
      fsdp_size: -1
  ppo_mini_batch_size: ${verifier.actor.ppo_mini_batch_size}
  ppo_micro_batch_size: null
  ppo_micro_batch_size_per_gpu: null
  forward_micro_batch_size: ${verifier_critic.ppo_micro_batch_size}
  forward_micro_batch_size_per_gpu: ${verifier_critic.ppo_micro_batch_size_per_gpu}
  use_dynamic_bsz: ${verifier.actor.use_dynamic_bsz}
  ppo_max_token_len_per_gpu: 32768
  forward_max_token_len_per_gpu: ${verifier_critic.ppo_max_token_len_per_gpu}
  ulysses_sequence_parallel_size: 1
  ppo_epochs: ${verifier.actor.ppo_epochs}
  shuffle: ${verifier.actor.shuffle}
  grad_clip: 1.0
  cliprange_value: 0.5
  checkpoint:
    contents: ['model', 'hf_model', 'optimizer', 'extra']

verifier_algorithm:
  gamma: 1.0
  lam: 1.0
  adv_estimator: gae
  kl_penalty: kl
  kl_ctrl:
    type: fixed
    kl_coef: 0.001
  verifier_correctness_reward_weight: 1.0
  verifier_format_reward_weight: 1.0
  verifier_label_ema_decay: 0.8
  reweight_verifier_rewards: false
  reweight_method: "inverse"

algorithm:
  ground_truth_weight: 1.0
  generator_format_reward_weight: 1.0
  generator_outcome_reward_weight: 1.0
  generator_process_reward_weight: 1.0
  alpha_schedule: "none"  # "none", "exp", "linear", or "cosine"
  alpha_start: 0.1
  alpha_min: 1e-3
  exp_target_frac: 1.0 

trainer:
  generator_warmup_steps: 0
  verifier_warmup_steps: 40
  alternating_frequency: 1
  n_generator_steps: 3
  n_verifier_steps: 1
  initial_mode: generator
  verifier_critic_warmup: 0
  max_verifier_ckpt_to_keep: null
  val_verifications_to_log_to_wandb: 10
  val_only: false
  val_before_train: true

reward_model:
  gain: 1.0
  bias: 0.0
  enable: False
  strategy: fsdp
  orm_reward_weight: 0.0
  model:
    path: null
    external_lib: ${actor_rollout_ref.model.external_lib}
    use_remove_padding: False
    fsdp_config:
      wrap_policy:
        min_num_params: 0
      param_offload: False
      fsdp_size: -1
  micro_batch_size: null # will be deprecated, use micro_batch_size_per_gpu
  micro_batch_size_per_gpu: null # set a number
  max_length: null
  ulysses_sequence_parallel_size: 1 # sp size
  use_dynamic_bsz: ${critic.use_dynamic_bsz}
  forward_max_token_len_per_gpu: ${critic.forward_max_token_len_per_gpu}
  reward_manager: naive